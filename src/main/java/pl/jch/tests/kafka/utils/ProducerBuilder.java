package pl.jch.tests.kafka.utils;

import java.util.HashMap;
import java.util.Map;

import io.confluent.kafka.serializers.AbstractKafkaSchemaSerDeConfig;
import io.confluent.kafka.serializers.KafkaAvroSerializerConfig;
import org.apache.kafka.clients.ClientDnsLookup;
import org.apache.kafka.clients.CommonClientConfigs;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.Producer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.common.config.SaslConfigs;
import org.apache.kafka.common.config.SslConfigs;
import org.apache.kafka.common.metrics.Sensor;
import org.apache.kafka.common.serialization.Serializer;
import org.apache.kafka.common.serialization.StringSerializer;

@SuppressWarnings("unused")
public class ProducerBuilder {

    public static final String DEFAULT_BOOTSTRAP_SERVERS = "localhost:9092";
    public static final String DEFAULT_SCHEMA_REFISTRY_URL = "http://localhost:8081";
    final Map<String, Object> config = new HashMap<>();

    public static ProducerBuilder builder() {
        return new ProducerBuilder()
                .bootstrapServers(DEFAULT_BOOTSTRAP_SERVERS)
                .schemaRegistryUrl(DEFAULT_SCHEMA_REFISTRY_URL)
                .acks(Acks.ALL)
                .keySerializer(StringSerializer.class)
                .valueSerializer(StringSerializer.class)
                .retries(0);
    }

    public static ProducerBuilder builder(Class<? extends Serializer<?>> keySerializerClass,
                                          Class<? extends Serializer<?>> valueSerializerClass) {
        return builder()
                .keySerializer(keySerializerClass)
                .valueSerializer(valueSerializerClass);
    }

    private ProducerBuilder config(String configName, Object configValue) {
        this.config.put(configName, configValue);
        return this;
    }

    // ### AUTOGENERATED BUILDER METHODS START ###
    // DO NOT EDIT MANUALLY

    /**
     * The number of acknowledgments the producer requires the leader to have received before considering a
     * request complete. This controls the  durability of records that are sent. The following settings are
     * allowed:  <ul> <li><code>acks=0</code> If set to zero then the producer will not wait for any
     * acknowledgment from the server at all. The record will be immediately added to the socket buffer and
     * considered sent. No guarantee can be made that the server has received the record in this case, and
     * the <code>retries</code> configuration will not take effect (as the client won't generally know of
     * any failures). The offset given back for each record will always be set to <code>-1</code>.
     * <li><code>acks=1</code> This will mean the leader will write the record to its local log but will
     * respond without awaiting full acknowledgement from all followers. In this case should the leader
     * fail immediately after acknowledging the record but before the followers have replicated it then the
     * record will be lost. <li><code>acks=all</code> This means the leader will wait for the full set of
     * in-sync replicas to acknowledge the record. This guarantees that the record will not be lost as long
     * as at least one in-sync replica remains alive. This is the strongest available guarantee. This is
     * equivalent to the acks=-1 setting.</ul>
     */
    public ProducerBuilder acks(Acks acks) {
        return config(ProducerConfig.ACKS_CONFIG, acks.getId());
    }

    /**
     * Specify if the Serializer should attempt to register the Schema with Schema Registry
     */
    public ProducerBuilder autoRegisterSchemas(boolean autoRegisterSchemas) {
        return config(AbstractKafkaSchemaSerDeConfig.AUTO_REGISTER_SCHEMAS, autoRegisterSchemas);
    }

    /**
     * If true, allows null field values used in ReflectionAvroSerializer
     */
    public ProducerBuilder avroReflectionAllowNull(boolean avroReflectionAllowNull) {
        return config(KafkaAvroSerializerConfig.AVRO_REFLECTION_ALLOW_NULL_CONFIG, avroReflectionAllowNull);
    }

    /**
     * Whether to remove Java-specific properties generated by Avro
     */
    public ProducerBuilder avroRemoveJavaProperties(boolean avroRemoveJavaProperties) {
        return config(KafkaAvroSerializerConfig.AVRO_REMOVE_JAVA_PROPS_CONFIG, avroRemoveJavaProperties);
    }

    /**
     * If true, use logical type converter in generic record
     */
    public ProducerBuilder avroUseLogicalTypeConverters(boolean avroUseLogicalTypeConverters) {
        return config(KafkaAvroSerializerConfig.AVRO_USE_LOGICAL_TYPE_CONVERTERS_CONFIG, avroUseLogicalTypeConverters);
    }

    /**
     * Specify how to pick the credentials for Basic Auth header. The supported values are URL, USER_INFO
     * and SASL_INHERIT
     */
    public ProducerBuilder basicAuthCredentialsSource(String basicAuthCredentialsSource) {
        return config(AbstractKafkaSchemaSerDeConfig.BASIC_AUTH_CREDENTIALS_SOURCE, basicAuthCredentialsSource);
    }

    /**
     * Specify the user info for Basic Auth in the form of {username}:{password}
     */
    public ProducerBuilder basicAuthUserInfo(String basicAuthUserInfo) {
        return config(AbstractKafkaSchemaSerDeConfig.USER_INFO_CONFIG, basicAuthUserInfo);
    }

    /**
     * The producer will attempt to batch records together into fewer requests whenever multiple records
     * are being sent to the same partition. This helps performance on both the client and the server. This
     * configuration controls the default batch size in bytes. <p>No attempt will be made to batch records
     * larger than this size. <p>Requests sent to brokers will contain multiple batches, one for each
     * partition with data available to be sent. <p>A small batch size will make batching less common and
     * may reduce throughput (a batch size of zero will disable batching entirely). A very large batch size
     * may use memory a bit more wastefully as we will always allocate a buffer of the specified batch size
     * in anticipation of additional records.
     */
    public ProducerBuilder batchSize(int batchSize) {
        return config(ProducerConfig.BATCH_SIZE_CONFIG, batchSize);
    }

    /**
     * Specify how to pick the credentials for Bearer Auth header.
     */
    public ProducerBuilder bearerAuthCredentialsSource(String bearerAuthCredentialsSource) {
        return config(AbstractKafkaSchemaSerDeConfig.BEARER_AUTH_CREDENTIALS_SOURCE, bearerAuthCredentialsSource);
    }

    /**
     * Specify the Bearer token to be used for authentication
     */
    public ProducerBuilder bearerAuthToken(String bearerAuthToken) {
        return config(AbstractKafkaSchemaSerDeConfig.BEARER_AUTH_TOKEN_CONFIG, bearerAuthToken);
    }

    /**
     * A list of host/port pairs to use for establishing the initial connection to the Kafka cluster. The
     * client will make use of all servers irrespective of which servers are specified here for
     * bootstrapping&mdash;this list only impacts the initial hosts used to discover the full set of
     * servers. This list should be in the form <code>host1:port1,host2:port2,...</code>. Since these
     * servers are just used for the initial connection to discover the full cluster membership (which may
     * change dynamically), this list need not contain the full set of servers (you may want more than one,
     * though, in case a server is down).
     */
    public ProducerBuilder bootstrapServers(String bootstrapServers) {
        return config(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
    }

    /**
     * The total bytes of memory the producer can use to buffer records waiting to be sent to the server.
     * If records are sent faster than they can be delivered to the server the producer will block for
     * <code>max.block.ms</code> after which it will throw an exception.<p>This setting should correspond
     * roughly to the total memory the producer will use, but is not a hard bound since not all memory the
     * producer uses is used for buffering. Some additional memory will be used for compression (if
     * compression is enabled) as well as for maintaining in-flight requests.
     */
    public ProducerBuilder bufferMemory(long bufferMemory) {
        return config(ProducerConfig.BUFFER_MEMORY_CONFIG, bufferMemory);
    }

    /**
     * Controls how the client uses DNS lookups. If set to <code>use_all_dns_ips</code>, connect to each
     * returned IP address in sequence until a successful connection is established. After a disconnection,
     * the next IP is used. Once all IPs have been used once, the client resolves the IP(s) from the
     * hostname again (both the JVM and the OS cache DNS name lookups, however). If set to
     * <code>resolve_canonical_bootstrap_servers_only</code>, resolve each bootstrap address into a list of
     * canonical names. After the bootstrap phase, this behaves the same as <code>use_all_dns_ips</code>.
     * If set to <code>default</code> (deprecated), attempt to connect to the first IP address returned by
     * the lookup, even if the lookup returns multiple IP addresses.
     */
    public ProducerBuilder clientDnsLookup(ClientDnsLookup clientDnsLookup) {
        return config(ProducerConfig.CLIENT_DNS_LOOKUP_CONFIG, clientDnsLookup);
    }

    /**
     * An id string to pass to the server when making requests. The purpose of this is to be able to track
     * the source of requests beyond just ip/port by allowing a logical application name to be included in
     * server-side request logging.
     */
    public ProducerBuilder clientId(String clientId) {
        return config(ProducerConfig.CLIENT_ID_CONFIG, clientId);
    }

    /**
     * The compression type for all data generated by the producer. The default is none (i.e. no
     * compression). Valid  values are <code>none</code>, <code>gzip</code>, <code>snappy</code>,
     * <code>lz4</code>, or <code>zstd</code>. Compression is of full batches of data, so the efficacy of
     * batching will also impact the compression ratio (more batching means better compression).
     */
    public ProducerBuilder compressionType(String compressionType) {
        return config(ProducerConfig.COMPRESSION_TYPE_CONFIG, compressionType);
    }

    /**
     * Close idle connections after the number of milliseconds specified by this config.
     */
    public ProducerBuilder connectionsMaxIdleMs(long connectionsMaxIdleMs) {
        return config(ProducerConfig.CONNECTIONS_MAX_IDLE_MS_CONFIG, connectionsMaxIdleMs);
    }

    /**
     * An upper bound on the time to report success or failure after a call to <code>send()</code> returns.
     * This limits the total time that a record will be delayed prior to sending, the time to await
     * acknowledgement from the broker (if expected), and the time allowed for retriable send failures. The
     * producer may report failure to send a record earlier than this config if either an unrecoverable
     * error is encountered, the retries have been exhausted, or the record is added to a batch which
     * reached an earlier delivery expiration deadline. The value of this config should be greater than or
     * equal to the sum of <code>request.timeout.ms</code> and <code>linger.ms</code>.
     */
    public ProducerBuilder deliveryTimeoutMs(int deliveryTimeoutMs) {
        return config(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG, deliveryTimeoutMs);
    }

    /**
     * When set to 'true', the producer will ensure that exactly one copy of each message is written in the
     * stream. If 'false', producer retries due to broker failures, etc., may write duplicates of the
     * retried message in the stream. Note that enabling idempotence requires
     * <code>max.in.flight.requests.per.connection</code> to be less than or equal to 5,
     * <code>retries</code> to be greater than 0 and <code>acks</code> must be 'all'. If these values are
     * not explicitly set by the user, suitable values will be chosen. If incompatible values are set, a
     * <code>ConfigException</code> will be thrown.
     */
    public ProducerBuilder enableIdempotence(boolean enableIdempotence) {
        return config(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, enableIdempotence);
    }

    /**
     * A list of classes to use as interceptors. Implementing the
     * <code>org.apache.kafka.clients.producer.ProducerInterceptor</code> interface allows you to intercept
     * (and possibly mutate) the records received by the producer before they are published to the Kafka
     * cluster. By default, there are no interceptors.
     */
    public ProducerBuilder interceptorClasses(String interceptorClasses) {
        return config(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, interceptorClasses);
    }

    public ProducerBuilder internalAutoDowngradeTxnCommit(boolean internalAutoDowngradeTxnCommit) {
        return config("internal.auto.downgrade.txn.commit", internalAutoDowngradeTxnCommit);
    }

    /**
     * Serializer class for key that implements the
     * <code>org.apache.kafka.common.serialization.Serializer</code> interface.
     */
    public ProducerBuilder keySerializer(Class<? extends Serializer<?>> keySerializer) {
        return config(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, keySerializer);
    }

    /**
     * Determines how to construct the subject name under which the key schema is registered with the
     * schema registry. By default, <topic>-key is used as subject.
     */
    public ProducerBuilder keySubjectNameStrategy(Class<?> keySubjectNameStrategy) {
        return config(AbstractKafkaSchemaSerDeConfig.KEY_SUBJECT_NAME_STRATEGY, keySubjectNameStrategy);
    }

    /**
     * Whether to check for backward compatibility between the latest subject version and  the Schema of
     * the object to be serialized
     */
    public ProducerBuilder latestCompatibilityStrict(boolean latestCompatibilityStrict) {
        return config(AbstractKafkaSchemaSerDeConfig.LATEST_COMPATIBILITY_STRICT, latestCompatibilityStrict);
    }

    /**
     * The producer groups together any records that arrive in between request transmissions into a single
     * batched request. Normally this occurs only under load when records arrive faster than they can be
     * sent out. However in some circumstances the client may want to reduce the number of requests even
     * under moderate load. This setting accomplishes this by adding a small amount of artificial
     * delay&mdash;that is, rather than immediately sending out a record the producer will wait for up to
     * the given delay to allow other records to be sent so that the sends can be batched together. This
     * can be thought of as analogous to Nagle's algorithm in TCP. This setting gives the upper bound on
     * the delay for batching: once we get <code>batch.size</code> worth of records for a partition it will
     * be sent immediately regardless of this setting, however if we have fewer than this many bytes
     * accumulated for this partition we will 'linger' for the specified time waiting for more records to
     * show up. This setting defaults to 0 (i.e. no delay). Setting <code>linger.ms=5</code>, for example,
     * would have the effect of reducing the number of requests sent but would add up to 5ms of latency to
     * records sent in the absence of load.
     */
    public ProducerBuilder lingerMs(long lingerMs) {
        return config(ProducerConfig.LINGER_MS_CONFIG, lingerMs);
    }

    /**
     * The configuration controls how long the <code>KafkaProducer</code>'s <code>send()</code>,
     * <code>partitionsFor()</code>, <code>initTransactions()</code>,
     * <code>sendOffsetsToTransaction()</code>, <code>commitTransaction()</code> and
     * <code>abortTransaction()</code> methods will block. For <code>send()</code> this timeout bounds the
     * total time waiting for both metadata fetch and buffer allocation (blocking in the user-supplied
     * serializers or partitioner is not counted against this timeout). For <code>partitionsFor()</code>
     * this timeout bounds the time spent waiting for metadata if it is unavailable. The
     * transaction-related methods always block, but may timeout if the transaction coordinator could not
     * be discovered or did not respond within the timeout.
     */
    public ProducerBuilder maxBlockMs(long maxBlockMs) {
        return config(ProducerConfig.MAX_BLOCK_MS_CONFIG, maxBlockMs);
    }

    /**
     * The maximum number of unacknowledged requests the client will send on a single connection before
     * blocking. Note that if this setting is set to be greater than 1 and there are failed sends, there is
     * a risk of message re-ordering due to retries (i.e., if retries are enabled).
     */
    public ProducerBuilder maxInFlightRequestsPerConnection(int maxInFlightRequestsPerConnection) {
        return config(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, maxInFlightRequestsPerConnection);
    }

    /**
     * The maximum size of a request in bytes. This setting will limit the number of record batches the
     * producer will send in a single request to avoid sending huge requests. This is also effectively a
     * cap on the maximum uncompressed record batch size. Note that the server has its own cap on the
     * record batch size (after compression if compression is enabled) which may be different from this.
     */
    public ProducerBuilder maxRequestSize(int maxRequestSize) {
        return config(ProducerConfig.MAX_REQUEST_SIZE_CONFIG, maxRequestSize);
    }

    /**
     * Maximum number of schemas to create or cache locally.
     */
    public ProducerBuilder maxSchemasPerSubject(int maxSchemasPerSubject) {
        return config(AbstractKafkaSchemaSerDeConfig.MAX_SCHEMAS_PER_SUBJECT_CONFIG, maxSchemasPerSubject);
    }

    /**
     * The period of time in milliseconds after which we force a refresh of metadata even if we haven't
     * seen any partition leadership changes to proactively discover any new brokers or partitions.
     */
    public ProducerBuilder metadataMaxAgeMs(long metadataMaxAgeMs) {
        return config(ProducerConfig.METADATA_MAX_AGE_CONFIG, metadataMaxAgeMs);
    }

    /**
     * Controls how long the producer will cache metadata for a topic that's idle. If the elapsed time
     * since a topic was last produced to exceeds the metadata idle duration, then the topic's metadata is
     * forgotten and the next access to it will force a metadata fetch request.
     */
    public ProducerBuilder metadataMaxIdleMs(long metadataMaxIdleMs) {
        return config(ProducerConfig.METADATA_MAX_IDLE_CONFIG, metadataMaxIdleMs);
    }

    /**
     * A list of classes to use as metrics reporters. Implementing the
     * <code>org.apache.kafka.common.metrics.MetricsReporter</code> interface allows plugging in classes
     * that will be notified of new metric creation. The JmxReporter is always included to register JMX
     * statistics.
     */
    public ProducerBuilder metricReporters(String metricReporters) {
        return config(ProducerConfig.METRIC_REPORTER_CLASSES_CONFIG, metricReporters);
    }

    /**
     * The number of samples maintained to compute metrics.
     */
    public ProducerBuilder metricsNumSamples(int metricsNumSamples) {
        return config(ProducerConfig.METRICS_NUM_SAMPLES_CONFIG, metricsNumSamples);
    }

    /**
     * The highest recording level for metrics.
     */
    public ProducerBuilder metricsRecordingLevel(Sensor.RecordingLevel metricsRecordingLevel) {
        return config(ProducerConfig.METRICS_RECORDING_LEVEL_CONFIG, metricsRecordingLevel);
    }

    /**
     * The window of time a metrics sample is computed over.
     */
    public ProducerBuilder metricsSampleWindowMs(long metricsSampleWindowMs) {
        return config(ProducerConfig.METRICS_SAMPLE_WINDOW_MS_CONFIG, metricsSampleWindowMs);
    }

    /**
     * Partitioner class that implements the <code>org.apache.kafka.clients.producer.Partitioner</code>
     * interface.
     */
    public ProducerBuilder partitionerClass(Class<?> partitionerClass) {
        return config(ProducerConfig.PARTITIONER_CLASS_CONFIG, partitionerClass);
    }

    /**
     * The hostname, or address, of the proxy server that will be used to connect to the schema registry
     * instances.
     */
    public ProducerBuilder proxyHost(String proxyHost) {
        return config(AbstractKafkaSchemaSerDeConfig.PROXY_HOST, proxyHost);
    }

    /**
     * The port number of the proxy server that will be used to connect to the schema registry instances.
     */
    public ProducerBuilder proxyPort(int proxyPort) {
        return config(AbstractKafkaSchemaSerDeConfig.PROXY_PORT, proxyPort);
    }

    /**
     * The size of the TCP receive buffer (SO_RCVBUF) to use when reading data. If the value is -1, the OS
     * default will be used.
     */
    public ProducerBuilder receiveBufferBytes(int receiveBufferBytes) {
        return config(ProducerConfig.RECEIVE_BUFFER_CONFIG, receiveBufferBytes);
    }

    /**
     * The maximum amount of time in milliseconds to wait when reconnecting to a broker that has repeatedly
     * failed to connect. If provided, the backoff per host will increase exponentially for each
     * consecutive connection failure, up to this maximum. After calculating the backoff increase, 20%
     * random jitter is added to avoid connection storms.
     */
    public ProducerBuilder reconnectBackoffMaxMs(long reconnectBackoffMaxMs) {
        return config(ProducerConfig.RECONNECT_BACKOFF_MAX_MS_CONFIG, reconnectBackoffMaxMs);
    }

    /**
     * The base amount of time to wait before attempting to reconnect to a given host. This avoids
     * repeatedly connecting to a host in a tight loop. This backoff applies to all connection attempts by
     * the client to a broker.
     */
    public ProducerBuilder reconnectBackoffMs(long reconnectBackoffMs) {
        return config(ProducerConfig.RECONNECT_BACKOFF_MS_CONFIG, reconnectBackoffMs);
    }

    /**
     * The configuration controls the maximum amount of time the client will wait for the response of a
     * request. If the response is not received before the timeout elapses the client will resend the
     * request if necessary or fail the request if retries are exhausted. This should be larger than
     * <code>replica.lag.time.max.ms</code> (a broker configuration) to reduce the possibility of message
     * duplication due to unnecessary producer retries.
     */
    public ProducerBuilder requestTimeoutMs(int requestTimeoutMs) {
        return config(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG, requestTimeoutMs);
    }

    /**
     * Setting a value greater than zero will cause the client to resend any record whose send fails with a
     * potentially transient error. Note that this retry is no different than if the client resent the
     * record upon receiving the error. Allowing retries without setting
     * <code>max.in.flight.requests.per.connection</code> to 1 will potentially change the ordering of
     * records because if two batches are sent to a single partition, and the first fails and is retried
     * but the second succeeds, then the records in the second batch may appear first. Note additionally
     * that produce requests will be failed before the number of retries has been exhausted if the timeout
     * configured by <code>delivery.timeout.ms</code> expires first before successful acknowledgement.
     * Users should generally prefer to leave this config unset and instead use
     * <code>delivery.timeout.ms</code> to control retry behavior.
     */
    public ProducerBuilder retries(int retries) {
        return config(ProducerConfig.RETRIES_CONFIG, retries);
    }

    /**
     * The amount of time to wait before attempting to retry a failed request to a given topic partition.
     * This avoids repeatedly sending requests in a tight loop under some failure scenarios.
     */
    public ProducerBuilder retryBackoffMs(long retryBackoffMs) {
        return config(ProducerConfig.RETRY_BACKOFF_MS_CONFIG, retryBackoffMs);
    }

    /**
     * The fully qualified name of a SASL client callback handler class that implements the
     * AuthenticateCallbackHandler interface.
     */
    public ProducerBuilder saslClientCallbackHandlerClass(Class<?> saslClientCallbackHandlerClass) {
        return config(SaslConfigs.SASL_CLIENT_CALLBACK_HANDLER_CLASS, saslClientCallbackHandlerClass);
    }

    /**
     * JAAS login context parameters for SASL connections in the format used by JAAS configuration files.
     * JAAS configuration file format is described <a
     * href="http://docs.oracle.com/javase/8/docs/technotes/guides/security/jgss/tutorials/LoginConfigFile.html">here</a>.
     * The format for the value is: <code>loginModuleClass controlFlag (optionName=optionValue)*;</code>.
     * For brokers, the config must be prefixed with listener prefix and SASL mechanism name in lower-case.
     * For example, listener.name.sasl_ssl.scram-sha-256.sasl.jaas.config=com.example.ScramLoginModule
     * required;
     */
    public ProducerBuilder saslJaasConfig(String saslJaasConfig) {
        return config(SaslConfigs.SASL_JAAS_CONFIG, saslJaasConfig);
    }

    /**
     * Kerberos kinit command path.
     */
    public ProducerBuilder saslKerberosKinitCmd(String saslKerberosKinitCmd) {
        return config(SaslConfigs.SASL_KERBEROS_KINIT_CMD, saslKerberosKinitCmd);
    }

    /**
     * Login thread sleep time between refresh attempts.
     */
    public ProducerBuilder saslKerberosMinTimeBeforeRelogin(long saslKerberosMinTimeBeforeRelogin) {
        return config(SaslConfigs.SASL_KERBEROS_MIN_TIME_BEFORE_RELOGIN, saslKerberosMinTimeBeforeRelogin);
    }

    /**
     * The Kerberos principal name that Kafka runs as. This can be defined either in Kafka's JAAS config or
     * in Kafka's config.
     */
    public ProducerBuilder saslKerberosServiceName(String saslKerberosServiceName) {
        return config(SaslConfigs.SASL_KERBEROS_SERVICE_NAME, saslKerberosServiceName);
    }

    /**
     * Percentage of random jitter added to the renewal time.
     */
    public ProducerBuilder saslKerberosTicketRenewJitter(double saslKerberosTicketRenewJitter) {
        return config(SaslConfigs.SASL_KERBEROS_TICKET_RENEW_JITTER, saslKerberosTicketRenewJitter);
    }

    /**
     * Login thread will sleep until the specified window factor of time from last refresh to ticket's
     * expiry has been reached, at which time it will try to renew the ticket.
     */
    public ProducerBuilder saslKerberosTicketRenewWindowFactor(double saslKerberosTicketRenewWindowFactor) {
        return config(SaslConfigs.SASL_KERBEROS_TICKET_RENEW_WINDOW_FACTOR, saslKerberosTicketRenewWindowFactor);
    }

    /**
     * The fully qualified name of a SASL login callback handler class that implements the
     * AuthenticateCallbackHandler interface. For brokers, login callback handler config must be prefixed
     * with listener prefix and SASL mechanism name in lower-case. For example,
     * listener.name.sasl_ssl.scram-sha-256.sasl.login.callback.handler.class=com.example.CustomScramLoginCallbackHandler
     */
    public ProducerBuilder saslLoginCallbackHandlerClass(Class<?> saslLoginCallbackHandlerClass) {
        return config(SaslConfigs.SASL_LOGIN_CALLBACK_HANDLER_CLASS, saslLoginCallbackHandlerClass);
    }

    /**
     * The fully qualified name of a class that implements the Login interface. For brokers, login config
     * must be prefixed with listener prefix and SASL mechanism name in lower-case. For example,
     * listener.name.sasl_ssl.scram-sha-256.sasl.login.class=com.example.CustomScramLogin
     */
    public ProducerBuilder saslLoginClass(Class<?> saslLoginClass) {
        return config(SaslConfigs.SASL_LOGIN_CLASS, saslLoginClass);
    }

    /**
     * The amount of buffer time before credential expiration to maintain when refreshing a credential, in
     * seconds. If a refresh would otherwise occur closer to expiration than the number of buffer seconds
     * then the refresh will be moved up to maintain as much of the buffer time as possible. Legal values
     * are between 0 and 3600 (1 hour); a default value of  300 (5 minutes) is used if no value is
     * specified. This value and sasl.login.refresh.min.period.seconds are both ignored if their sum
     * exceeds the remaining lifetime of a credential. Currently applies only to OAUTHBEARER.
     */
    public ProducerBuilder saslLoginRefreshBufferSeconds(short saslLoginRefreshBufferSeconds) {
        return config(SaslConfigs.SASL_LOGIN_REFRESH_BUFFER_SECONDS, saslLoginRefreshBufferSeconds);
    }

    /**
     * The desired minimum time for the login refresh thread to wait before refreshing a credential, in
     * seconds. Legal values are between 0 and 900 (15 minutes); a default value of 60 (1 minute) is used
     * if no value is specified.  This value and  sasl.login.refresh.buffer.seconds are both ignored if
     * their sum exceeds the remaining lifetime of a credential. Currently applies only to OAUTHBEARER.
     */
    public ProducerBuilder saslLoginRefreshMinPeriodSeconds(short saslLoginRefreshMinPeriodSeconds) {
        return config(SaslConfigs.SASL_LOGIN_REFRESH_MIN_PERIOD_SECONDS, saslLoginRefreshMinPeriodSeconds);
    }

    /**
     * Login refresh thread will sleep until the specified window factor relative to the credential's
     * lifetime has been reached, at which time it will try to refresh the credential. Legal values are
     * between 0.5 (50%) and 1.0 (100%) inclusive; a default value of 0.8 (80%) is used if no value is
     * specified. Currently applies only to OAUTHBEARER.
     */
    public ProducerBuilder saslLoginRefreshWindowFactor(double saslLoginRefreshWindowFactor) {
        return config(SaslConfigs.SASL_LOGIN_REFRESH_WINDOW_FACTOR, saslLoginRefreshWindowFactor);
    }

    /**
     * The maximum amount of random jitter relative to the credential's lifetime that is added to the login
     * refresh thread's sleep time. Legal values are between 0 and 0.25 (25%) inclusive; a default value of
     * 0.05 (5%) is used if no value is specified. Currently applies only to OAUTHBEARER.
     */
    public ProducerBuilder saslLoginRefreshWindowJitter(double saslLoginRefreshWindowJitter) {
        return config(SaslConfigs.SASL_LOGIN_REFRESH_WINDOW_JITTER, saslLoginRefreshWindowJitter);
    }

    /**
     * SASL mechanism used for client connections. This may be any mechanism for which a security provider
     * is available. GSSAPI is the default mechanism.
     */
    public ProducerBuilder saslMechanism(String saslMechanism) {
        return config(SaslConfigs.SASL_MECHANISM, saslMechanism);
    }

    /**
     * If true, uses the reflection API when serializing/deserializing
     */
    public ProducerBuilder schemaReflection(boolean schemaReflection) {
        return config(AbstractKafkaSchemaSerDeConfig.SCHEMA_REFLECTION_CONFIG, schemaReflection);
    }

    /**
     * Specify the user info for Basic Auth in the form of {username}:{password}
     */
    public ProducerBuilder schemaRegistryBasicAuthUserInfo(String schemaRegistryBasicAuthUserInfo) {
        return config(AbstractKafkaSchemaSerDeConfig.SCHEMA_REGISTRY_USER_INFO_CONFIG, schemaRegistryBasicAuthUserInfo);
    }

    /**
     * A list of cipher suites. This is a named combination of authentication, encryption, MAC and key
     * exchange algorithm used to negotiate the security settings for a network connection using TLS or SSL
     * network protocol. By default all the available cipher suites are supported.
     */
    public ProducerBuilder schemaRegistrySslCipherSuites(String schemaRegistrySslCipherSuites) {
        return config("schema.registry.ssl.cipher.suites", schemaRegistrySslCipherSuites);
    }

    /**
     * The list of protocols enabled for SSL connections. The default is 'TLSv1.2,TLSv1.3' when running
     * with Java 11 or newer, 'TLSv1.2' otherwise. With the default value for Java 11, clients and servers
     * will prefer TLSv1.3 if both support it and fallback to TLSv1.2 otherwise (assuming both support at
     * least TLSv1.2). This default should be fine for most cases. Also see the config documentation for
     * `ssl.protocol`.
     */
    public ProducerBuilder schemaRegistrySslEnabledProtocols(String schemaRegistrySslEnabledProtocols) {
        return config("schema.registry.ssl.enabled.protocols", schemaRegistrySslEnabledProtocols);
    }

    /**
     * The endpoint identification algorithm to validate server hostname using server certificate.
     */
    public ProducerBuilder schemaRegistrySslEndpointIdentificationAlgorithm(String schemaRegistrySslEndpointIdentificationAlgorithm) {
        return config("schema.registry.ssl.endpoint.identification.algorithm", schemaRegistrySslEndpointIdentificationAlgorithm);
    }

    /**
     * The class of type org.apache.kafka.common.security.auth.SslEngineFactory to provide SSLEngine
     * objects. Default value is org.apache.kafka.common.security.ssl.DefaultSslEngineFactory
     */
    public ProducerBuilder schemaRegistrySslEngineFactoryClass(Class<?> schemaRegistrySslEngineFactoryClass) {
        return config("schema.registry.ssl.engine.factory.class", schemaRegistrySslEngineFactoryClass);
    }

    /**
     * The password of the private key in the key store file orthe PEM key specified in `ssl.keystore.key'.
     * This is required for clients only if two-way authentication is configured.
     */
    public ProducerBuilder schemaRegistrySslKeyPassword(String schemaRegistrySslKeyPassword) {
        return config("schema.registry.ssl.key.password", schemaRegistrySslKeyPassword);
    }

    /**
     * The algorithm used by key manager factory for SSL connections. Default value is the key manager
     * factory algorithm configured for the Java Virtual Machine.
     */
    public ProducerBuilder schemaRegistrySslKeymanagerAlgorithm(String schemaRegistrySslKeymanagerAlgorithm) {
        return config("schema.registry.ssl.keymanager.algorithm", schemaRegistrySslKeymanagerAlgorithm);
    }

    /**
     * Certificate chain in the format specified by 'ssl.keystore.type'. Default SSL engine factory
     * supports only PEM format with a list of X.509 certificates
     */
    public ProducerBuilder schemaRegistrySslKeystoreCertificateChain(String schemaRegistrySslKeystoreCertificateChain) {
        return config("schema.registry.ssl.keystore.certificate.chain", schemaRegistrySslKeystoreCertificateChain);
    }

    /**
     * Private key in the format specified by 'ssl.keystore.type'. Default SSL engine factory supports only
     * PEM format with PKCS#8 keys. If the key is encrypted, key password must be specified using
     * 'ssl.key.password'
     */
    public ProducerBuilder schemaRegistrySslKeystoreKey(String schemaRegistrySslKeystoreKey) {
        return config("schema.registry.ssl.keystore.key", schemaRegistrySslKeystoreKey);
    }

    /**
     * The location of the key store file. This is optional for client and can be used for two-way
     * authentication for client.
     */
    public ProducerBuilder schemaRegistrySslKeystoreLocation(String schemaRegistrySslKeystoreLocation) {
        return config("schema.registry.ssl.keystore.location", schemaRegistrySslKeystoreLocation);
    }

    /**
     * The store password for the key store file. This is optional for client and only needed if
     * 'ssl.keystore.location' is configured.  Key store password is not supported for PEM format.
     */
    public ProducerBuilder schemaRegistrySslKeystorePassword(String schemaRegistrySslKeystorePassword) {
        return config("schema.registry.ssl.keystore.password", schemaRegistrySslKeystorePassword);
    }

    /**
     * The file format of the key store file. This is optional for client.
     */
    public ProducerBuilder schemaRegistrySslKeystoreType(String schemaRegistrySslKeystoreType) {
        return config("schema.registry.ssl.keystore.type", schemaRegistrySslKeystoreType);
    }

    /**
     * The SSL protocol used to generate the SSLContext. The default is 'TLSv1.3' when running with Java 11
     * or newer, 'TLSv1.2' otherwise. This value should be fine for most use cases. Allowed values in
     * recent JVMs are 'TLSv1.2' and 'TLSv1.3'. 'TLS', 'TLSv1.1', 'SSL', 'SSLv2' and 'SSLv3' may be
     * supported in older JVMs, but their usage is discouraged due to known security vulnerabilities. With
     * the default value for this config and 'ssl.enabled.protocols', clients will downgrade to 'TLSv1.2'
     * if the server does not support 'TLSv1.3'. If this config is set to 'TLSv1.2', clients will not use
     * 'TLSv1.3' even if it is one of the values in ssl.enabled.protocols and the server only supports
     * 'TLSv1.3'.
     */
    public ProducerBuilder schemaRegistrySslProtocol(String schemaRegistrySslProtocol) {
        return config("schema.registry.ssl.protocol", schemaRegistrySslProtocol);
    }

    /**
     * The name of the security provider used for SSL connections. Default value is the default security
     * provider of the JVM.
     */
    public ProducerBuilder schemaRegistrySslProvider(String schemaRegistrySslProvider) {
        return config("schema.registry.ssl.provider", schemaRegistrySslProvider);
    }

    /**
     * The SecureRandom PRNG implementation to use for SSL cryptography operations.
     */
    public ProducerBuilder schemaRegistrySslSecureRandomImplementation(String schemaRegistrySslSecureRandomImplementation) {
        return config("schema.registry.ssl.secure.random.implementation", schemaRegistrySslSecureRandomImplementation);
    }

    /**
     * The algorithm used by trust manager factory for SSL connections. Default value is the trust manager
     * factory algorithm configured for the Java Virtual Machine.
     */
    public ProducerBuilder schemaRegistrySslTrustmanagerAlgorithm(String schemaRegistrySslTrustmanagerAlgorithm) {
        return config("schema.registry.ssl.trustmanager.algorithm", schemaRegistrySslTrustmanagerAlgorithm);
    }

    /**
     * Trusted certificates in the format specified by 'ssl.truststore.type'. Default SSL engine factory
     * supports only PEM format with X.509 certificates.
     */
    public ProducerBuilder schemaRegistrySslTruststoreCertificates(String schemaRegistrySslTruststoreCertificates) {
        return config("schema.registry.ssl.truststore.certificates", schemaRegistrySslTruststoreCertificates);
    }

    /**
     * The location of the trust store file.
     */
    public ProducerBuilder schemaRegistrySslTruststoreLocation(String schemaRegistrySslTruststoreLocation) {
        return config("schema.registry.ssl.truststore.location", schemaRegistrySslTruststoreLocation);
    }

    /**
     * The password for the trust store file. If a password is not set, trust store file configured will
     * still be used, but integrity checking is disabled. Trust store password is not supported for PEM
     * format.
     */
    public ProducerBuilder schemaRegistrySslTruststorePassword(String schemaRegistrySslTruststorePassword) {
        return config("schema.registry.ssl.truststore.password", schemaRegistrySslTruststorePassword);
    }

    /**
     * The file format of the trust store file.
     */
    public ProducerBuilder schemaRegistrySslTruststoreType(String schemaRegistrySslTruststoreType) {
        return config("schema.registry.ssl.truststore.type", schemaRegistrySslTruststoreType);
    }

    /**
     * Comma-separated list of URLs for schema registry instances that can be used to register or look up
     * schemas. If you wish to get a connection to a mocked schema registry for testing, you can specify a
     * scope using the 'mock://' pseudo-protocol. For example, 'mock://my-scope-name' corresponds to
     * 'MockSchemaRegistry.getClientForScope("my-scope-name")'.
     */
    public ProducerBuilder schemaRegistryUrl(String schemaRegistryUrl) {
        return config(AbstractKafkaSchemaSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, schemaRegistryUrl);
    }

    /**
     * Protocol used to communicate with brokers. Valid values are: PLAINTEXT, SSL, SASL_PLAINTEXT,
     * SASL_SSL.
     */
    public ProducerBuilder securityProtocol(String securityProtocol) {
        return config(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, securityProtocol);
    }

    /**
     * A list of configurable creator classes each returning a provider implementing security algorithms.
     * These classes should implement the
     * <code>org.apache.kafka.common.security.auth.SecurityProviderCreator</code> interface.
     */
    public ProducerBuilder securityProviders(String securityProviders) {
        return config(ProducerConfig.SECURITY_PROVIDERS_CONFIG, securityProviders);
    }

    /**
     * The size of the TCP send buffer (SO_SNDBUF) to use when sending data. If the value is -1, the OS
     * default will be used.
     */
    public ProducerBuilder sendBufferBytes(int sendBufferBytes) {
        return config(ProducerConfig.SEND_BUFFER_CONFIG, sendBufferBytes);
    }

    /**
     * The maximum amount of time the client will wait for the socket connection to be established. The
     * connection setup timeout will increase exponentially for each consecutive connection failure up to
     * this maximum. To avoid connection storms, a randomization factor of 0.2 will be applied to the
     * timeout resulting in a random range between 20% below and 20% above the computed value.
     */
    public ProducerBuilder socketConnectionSetupTimeoutMaxMs(long socketConnectionSetupTimeoutMaxMs) {
        return config(ProducerConfig.SOCKET_CONNECTION_SETUP_TIMEOUT_MAX_MS_CONFIG, socketConnectionSetupTimeoutMaxMs);
    }

    /**
     * The amount of time the client will wait for the socket connection to be established. If the
     * connection is not built before the timeout elapses, clients will close the socket channel.
     */
    public ProducerBuilder socketConnectionSetupTimeoutMs(long socketConnectionSetupTimeoutMs) {
        return config(ProducerConfig.SOCKET_CONNECTION_SETUP_TIMEOUT_MS_CONFIG, socketConnectionSetupTimeoutMs);
    }

    /**
     * A list of cipher suites. This is a named combination of authentication, encryption, MAC and key
     * exchange algorithm used to negotiate the security settings for a network connection using TLS or SSL
     * network protocol. By default all the available cipher suites are supported.
     */
    public ProducerBuilder sslCipherSuites(String sslCipherSuites) {
        return config(SslConfigs.SSL_CIPHER_SUITES_CONFIG, sslCipherSuites);
    }

    /**
     * The list of protocols enabled for SSL connections. The default is 'TLSv1.2,TLSv1.3' when running
     * with Java 11 or newer, 'TLSv1.2' otherwise. With the default value for Java 11, clients and servers
     * will prefer TLSv1.3 if both support it and fallback to TLSv1.2 otherwise (assuming both support at
     * least TLSv1.2). This default should be fine for most cases. Also see the config documentation for
     * `ssl.protocol`.
     */
    public ProducerBuilder sslEnabledProtocols(String sslEnabledProtocols) {
        return config(SslConfigs.SSL_ENABLED_PROTOCOLS_CONFIG, sslEnabledProtocols);
    }

    /**
     * The endpoint identification algorithm to validate server hostname using server certificate.
     */
    public ProducerBuilder sslEndpointIdentificationAlgorithm(String sslEndpointIdentificationAlgorithm) {
        return config(SslConfigs.SSL_ENDPOINT_IDENTIFICATION_ALGORITHM_CONFIG, sslEndpointIdentificationAlgorithm);
    }

    /**
     * The class of type org.apache.kafka.common.security.auth.SslEngineFactory to provide SSLEngine
     * objects. Default value is org.apache.kafka.common.security.ssl.DefaultSslEngineFactory
     */
    public ProducerBuilder sslEngineFactoryClass(Class<?> sslEngineFactoryClass) {
        return config(SslConfigs.SSL_ENGINE_FACTORY_CLASS_CONFIG, sslEngineFactoryClass);
    }

    /**
     * The password of the private key in the key store file orthe PEM key specified in `ssl.keystore.key'.
     * This is required for clients only if two-way authentication is configured.
     */
    public ProducerBuilder sslKeyPassword(String sslKeyPassword) {
        return config(SslConfigs.SSL_KEY_PASSWORD_CONFIG, sslKeyPassword);
    }

    /**
     * The algorithm used by key manager factory for SSL connections. Default value is the key manager
     * factory algorithm configured for the Java Virtual Machine.
     */
    public ProducerBuilder sslKeymanagerAlgorithm(String sslKeymanagerAlgorithm) {
        return config(SslConfigs.SSL_KEYMANAGER_ALGORITHM_CONFIG, sslKeymanagerAlgorithm);
    }

    /**
     * Certificate chain in the format specified by 'ssl.keystore.type'. Default SSL engine factory
     * supports only PEM format with a list of X.509 certificates
     */
    public ProducerBuilder sslKeystoreCertificateChain(String sslKeystoreCertificateChain) {
        return config(SslConfigs.SSL_KEYSTORE_CERTIFICATE_CHAIN_CONFIG, sslKeystoreCertificateChain);
    }

    /**
     * Private key in the format specified by 'ssl.keystore.type'. Default SSL engine factory supports only
     * PEM format with PKCS#8 keys. If the key is encrypted, key password must be specified using
     * 'ssl.key.password'
     */
    public ProducerBuilder sslKeystoreKey(String sslKeystoreKey) {
        return config(SslConfigs.SSL_KEYSTORE_KEY_CONFIG, sslKeystoreKey);
    }

    /**
     * The location of the key store file. This is optional for client and can be used for two-way
     * authentication for client.
     */
    public ProducerBuilder sslKeystoreLocation(String sslKeystoreLocation) {
        return config(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, sslKeystoreLocation);
    }

    /**
     * The store password for the key store file. This is optional for client and only needed if
     * 'ssl.keystore.location' is configured.  Key store password is not supported for PEM format.
     */
    public ProducerBuilder sslKeystorePassword(String sslKeystorePassword) {
        return config(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, sslKeystorePassword);
    }

    /**
     * The file format of the key store file. This is optional for client.
     */
    public ProducerBuilder sslKeystoreType(String sslKeystoreType) {
        return config(SslConfigs.SSL_KEYSTORE_TYPE_CONFIG, sslKeystoreType);
    }

    /**
     * The SSL protocol used to generate the SSLContext. The default is 'TLSv1.3' when running with Java 11
     * or newer, 'TLSv1.2' otherwise. This value should be fine for most use cases. Allowed values in
     * recent JVMs are 'TLSv1.2' and 'TLSv1.3'. 'TLS', 'TLSv1.1', 'SSL', 'SSLv2' and 'SSLv3' may be
     * supported in older JVMs, but their usage is discouraged due to known security vulnerabilities. With
     * the default value for this config and 'ssl.enabled.protocols', clients will downgrade to 'TLSv1.2'
     * if the server does not support 'TLSv1.3'. If this config is set to 'TLSv1.2', clients will not use
     * 'TLSv1.3' even if it is one of the values in ssl.enabled.protocols and the server only supports
     * 'TLSv1.3'.
     */
    public ProducerBuilder sslProtocol(String sslProtocol) {
        return config(SslConfigs.SSL_PROTOCOL_CONFIG, sslProtocol);
    }

    /**
     * The name of the security provider used for SSL connections. Default value is the default security
     * provider of the JVM.
     */
    public ProducerBuilder sslProvider(String sslProvider) {
        return config(SslConfigs.SSL_PROVIDER_CONFIG, sslProvider);
    }

    /**
     * The SecureRandom PRNG implementation to use for SSL cryptography operations.
     */
    public ProducerBuilder sslSecureRandomImplementation(String sslSecureRandomImplementation) {
        return config(SslConfigs.SSL_SECURE_RANDOM_IMPLEMENTATION_CONFIG, sslSecureRandomImplementation);
    }

    /**
     * The algorithm used by trust manager factory for SSL connections. Default value is the trust manager
     * factory algorithm configured for the Java Virtual Machine.
     */
    public ProducerBuilder sslTrustmanagerAlgorithm(String sslTrustmanagerAlgorithm) {
        return config(SslConfigs.SSL_TRUSTMANAGER_ALGORITHM_CONFIG, sslTrustmanagerAlgorithm);
    }

    /**
     * Trusted certificates in the format specified by 'ssl.truststore.type'. Default SSL engine factory
     * supports only PEM format with X.509 certificates.
     */
    public ProducerBuilder sslTruststoreCertificates(String sslTruststoreCertificates) {
        return config(SslConfigs.SSL_TRUSTSTORE_CERTIFICATES_CONFIG, sslTruststoreCertificates);
    }

    /**
     * The location of the trust store file.
     */
    public ProducerBuilder sslTruststoreLocation(String sslTruststoreLocation) {
        return config(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, sslTruststoreLocation);
    }

    /**
     * The password for the trust store file. If a password is not set, trust store file configured will
     * still be used, but integrity checking is disabled. Trust store password is not supported for PEM
     * format.
     */
    public ProducerBuilder sslTruststorePassword(String sslTruststorePassword) {
        return config(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, sslTruststorePassword);
    }

    /**
     * The file format of the trust store file.
     */
    public ProducerBuilder sslTruststoreType(String sslTruststoreType) {
        return config(SslConfigs.SSL_TRUSTSTORE_TYPE_CONFIG, sslTruststoreType);
    }

    /**
     * The maximum amount of time in ms that the transaction coordinator will wait for a transaction status
     * update from the producer before proactively aborting the ongoing transaction.If this value is larger
     * than the transaction.max.timeout.ms setting in the broker, the request will fail with a
     * <code>InvalidTxnTimeoutException</code> error.
     */
    public ProducerBuilder transactionTimeoutMs(int transactionTimeoutMs) {
        return config(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG, transactionTimeoutMs);
    }

    /**
     * The TransactionalId to use for transactional delivery. This enables reliability semantics which span
     * multiple producer sessions since it allows the client to guarantee that transactions using the same
     * TransactionalId have been completed prior to starting any new transactions. If no TransactionalId is
     * provided, then the producer is limited to idempotent delivery. If a TransactionalId is configured,
     * <code>enable.idempotence</code> is implied. By default the TransactionId is not configured, which
     * means transactions cannot be used. Note that, by default, transactions require a cluster of at least
     * three brokers which is the recommended setting for production; for development you can change this,
     * by adjusting broker setting <code>transaction.state.log.replication.factor</code>.
     */
    public ProducerBuilder transactionalId(String transactionalId) {
        return config(ProducerConfig.TRANSACTIONAL_ID_CONFIG, transactionalId);
    }

    /**
     * Specify if the Serializer should use the latest subject version for serialization
     */
    public ProducerBuilder useLatestVersion(boolean useLatestVersion) {
        return config(AbstractKafkaSchemaSerDeConfig.USE_LATEST_VERSION, useLatestVersion);
    }

    /**
     * Serializer class for value that implements the
     * <code>org.apache.kafka.common.serialization.Serializer</code> interface.
     */
    public ProducerBuilder valueSerializer(Class<? extends Serializer<?>> valueSerializer) {
        return config(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, valueSerializer);
    }

    /**
     * Determines how to construct the subject name under which the value schema is registered with the
     * schema registry. By default, <topic>-value is used as subject.
     */
    public ProducerBuilder valueSubjectNameStrategy(Class<?> valueSubjectNameStrategy) {
        return config(AbstractKafkaSchemaSerDeConfig.VALUE_SUBJECT_NAME_STRATEGY, valueSubjectNameStrategy);
    }

    // ### AUTOGENERATED BUILDER METHODS END ###

    public <KeyT, ValueT> Producer<KeyT, ValueT> build() {
        return new KafkaProducer<>(this.config);
    }

    public <KeyT, ValueT, ResultT> ResultT execute(ProducerCallback<KeyT, ValueT, ResultT> callback) {
        try (final Producer<KeyT, ValueT> producer = this.build()) {
            final ResultT result = callback.execute(producer);
            producer.flush();
            return result;
        } catch (Exception e) {
            if (e instanceof RuntimeException) {
                throw (RuntimeException) e;
            }
            throw new RuntimeException(e);
        }
    }

    public <KeyT, ValueT> void execute(ProducerCallbackVoid<KeyT, ValueT> callback) {
        execute((ProducerCallback<KeyT, ValueT, Void>) producer -> {
            callback.execute(producer);
            return null;
        });
    }

    public enum Acks implements IdentifiableEnum<String> {
        NO_WAIT("0"),
        ONLY_LEADER("1"),
        ALL("all");

        private final String id;

        Acks(String id) {
            this.id = id;
        }

        @Override
        public String getId() {
            return this.id;
        }
    }
}
